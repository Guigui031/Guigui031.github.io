---
title: "Metadata curation to evaluate the use of LLMs in the context of a systematic review"
description: "This project is about collecting metadata from articles in systematic reviews to create clean datasets. The developped program performs automatic data cleaning, definition, and alignment for all these articles. The datasets are used by a Mila-affiliated M.Sc. student to evaluate the use of LLMs in the screening phase of systematic review."
pubDate: 2023-06-01
heroImage: "/ProjetCurationMetadonnees.png"
badge: "Research"
tags: ["LLM", "Data Curation", "Systematic Review", "Mila", "Research"]
githubUrl: "https://github.com/geodes-sms/IFT3150-ProjetCurationMetadonnees"
category: "machine-learning"
titleFr: "Curation de métadonnées pour évaluer l'utilisation des LLM dans le contexte d'une revue systématique"
descriptionFr: "Ce projet consiste à collecter les métadonnées d'articles dans des revues systématiques pour créer des jeux de données propres. Le programme développé effectue un nettoyage automatique, une définition et un alignement des données pour tous ces articles. Les jeux de données sont utilisés par un étudiant à la maîtrise affilié à Mila pour évaluer l'utilisation des LLM dans la phase de sélection des revues systématiques."
badgeFr: "Recherche"
---

# Metadata Curation for LLM Evaluation in Systematic Reviews

A research collaboration project focused on preparing high-quality datasets to evaluate Large Language Models in systematic review processes.

## Project Context

This project was developed in collaboration with a Mila-affiliated M.Sc. student to support research on using Large Language Models in the screening phase of systematic reviews - a critical step in academic research methodology.

## Project Objectives

### Primary Goals
- **Dataset Creation**: Build clean, structured datasets from systematic review articles
- **Metadata Extraction**: Collect comprehensive metadata from research articles
- **Data Quality Assurance**: Ensure datasets meet research standards for LLM evaluation
- **Research Support**: Enable systematic evaluation of LLM capabilities in academic screening

## Technical Implementation

### Data Collection Pipeline
- **Automated Extraction**: Systematic collection of article metadata
- **Source Integration**: Handling multiple data sources and formats
- **Quality Control**: Validation mechanisms for data integrity

### Data Processing Features
- **Automatic Cleaning**: Intelligent removal of inconsistencies and errors
- **Data Definition**: Standardization of metadata fields and formats
- **Alignment Algorithms**: Matching and harmonizing data across sources
- **Schema Validation**: Ensuring compliance with research standards

### Research Integration
- **Mila Collaboration**: Direct support for ongoing M.Sc. research
- **LLM Evaluation**: Datasets specifically designed for model assessment
- **Systematic Review Context**: Understanding of domain-specific requirements

## Technical Stack

- **Python**: Core development language
- **Data Processing Libraries**: Pandas, NumPy for data manipulation
- **Natural Language Processing**: Text processing and metadata extraction
- **Research Tools**: Academic database APIs and processing tools

## Impact & Applications

### Research Contribution
- Enabling systematic evaluation of LLMs in academic contexts
- Supporting evidence-based assessment of AI capabilities
- Contributing to the advancement of automated systematic reviews

### Methodology Innovation
- Automated pipeline for research data preparation
- Quality assurance frameworks for research datasets
- Reproducible data processing workflows

## Collaboration

This project represents a successful collaboration between academic research (Mila) and practical software development, demonstrating the importance of high-quality data preparation in AI research.

## Future Research Directions

- Extension to additional systematic review domains
- Integration with other LLM evaluation frameworks
- Automated screening system development
- Real-time data processing capabilities